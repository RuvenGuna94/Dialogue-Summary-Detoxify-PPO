{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune FLAN-T5 with Reinforcement Learning (PPO) and PEFT to Generate Less-Toxic Summaries\n",
    "\n",
    "In this notebook, the objective is to fine-tune a FLAN-T5 model (from the [PEFT notebook](https://github.com/RuvenGuna94/Dialogue-Summary-PEFT-Fine-Tuning/tree/main)) to generate less toxic content with Meta AI's hate speech reward model. The reward model is a binary classifier that predicts either \"not hate\" or \"hate\" for the given text. Proximal Policy Optimization (PPO) is used to fine-tune and reduce the model's toxicity.\n",
    "\n",
    "## 1 - Set up Kernel and Required Dependencies\n",
    "\n",
    "Install the required packages to use PyTorch and Hugging Face transformers and datasets.The `TRL` library gives access to the PPO algorithm (`trainer` and `training` arguments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.17.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (18.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets==2.17.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets==2.17.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets==2.17.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets==2.17.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets==2.17.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets==2.17.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets==2.17.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets==2.17.0) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.0) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.17.0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from pandas->datasets==2.17.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from pandas->datasets==2.17.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from pandas->datasets==2.17.0) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers==4.47.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: evaluate==0.4.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: rouge_score==0.1.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: trl==0.11.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (0.11.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers==4.47.1) (4.67.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from evaluate==0.4.0) (2.17.0)\n",
      "Requirement already satisfied: dill in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from evaluate==0.4.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from evaluate==0.4.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from evaluate==0.4.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from evaluate==0.4.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0) (2023.10.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from evaluate==0.4.0) (0.18.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from rouge_score==0.1.2) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from rouge_score==0.1.2) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from rouge_score==0.1.2) (1.17.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from trl==0.11.3) (2.5.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from trl==0.11.3) (1.2.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from trl==0.11.3) (0.9.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets>=2.0.0->evaluate==0.4.0) (18.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets>=2.0.0->evaluate==0.4.0) (0.6)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from datasets>=2.0.0->evaluate==0.4.0) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests->transformers==4.47.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests->transformers==4.47.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests->transformers==4.47.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests->transformers==4.47.1) (2024.12.14)\n",
      "Requirement already satisfied: networkx in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from torch>=1.4.0->trl==0.11.3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from torch>=1.4.0->trl==0.11.3) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from torch>=1.4.0->trl==0.11.3) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from torch>=1.4.0->trl==0.11.3) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.11.3) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from tqdm>=4.27->transformers==4.47.1) (0.4.6)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from tyro>=0.5.11->trl==0.11.3) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from tyro>=0.5.11->trl==0.11.3) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from tyro>=0.5.11->trl==0.11.3) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from tyro>=0.5.11->trl==0.11.3) (4.4.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from accelerate->trl==0.11.3) (5.9.0)\n",
      "Requirement already satisfied: click in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from nltk->rouge_score==0.1.2) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from nltk->rouge_score==0.1.2) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from pandas->evaluate==0.4.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from pandas->evaluate==0.4.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from pandas->evaluate==0.4.0) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.18.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.3) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.3) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from jinja2->torch>=1.4.0->trl==0.11.3) (3.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.3) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/huggingface/peft.git\n",
      "  Cloning https://github.com/huggingface/peft.git to c:\\users\\rg255041\\appdata\\local\\temp\\pip-req-build-h6prsgus\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 6d458b300fc2ed82e19f796b53af4c97d03ea604\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (2.5.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (4.47.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (1.2.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (0.4.5)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from peft==0.14.1.dev0) (0.27.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.14.1.dev0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.14.1.dev0) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.14.1.dev0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.14.1.dev0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from torch>=1.13.0->peft==0.14.1.dev0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from torch>=1.13.0->peft==0.14.1.dev0) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from torch>=1.13.0->peft==0.14.1.dev0) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from torch>=1.13.0->peft==0.14.1.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.14.1.dev0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from tqdm->peft==0.14.1.dev0) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers->peft==0.14.1.dev0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from transformers->peft==0.14.1.dev0) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.14.1.dev0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft==0.14.1.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft==0.14.1.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft==0.14.1.dev0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rg255041\\appdata\\local\\anaconda3\\envs\\peft\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft==0.14.1.dev0) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git 'C:\\Users\\rg255041\\AppData\\Local\\Temp\\pip-req-build-h6prsgus'\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets==2.17.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==2.5.1  \\\n",
    "    torchdata==0.10.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.47.1 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    trl==0.11.3\n",
    "\n",
    "# Installing the Reinforcement Learning library directly from github.\n",
    "%pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm library makes the loops show a smart progress meter.\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load FLAN-T5 Model, Prepare Reward Model and Toxicity Evaluator\n",
    "\n",
    "### 2.1 - Load Data and FLAN-T5 Model Fine-Tuned with Summarization Instruction\n",
    "\n",
    "The same Hugging Face dataset [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) and the pre-trained model [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) are utilized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=\"google/flan-t5-base\"\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset_original = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he next step involves preprocessing the dataset. A subset is taken, filtering the dialogues by length to ensure examples are long enough while remaining easy to read. Each dialogue is then wrapped with an instruction and tokenized. The token IDs are saved in the field `input_ids`, and the decoded prompts are saved in the field `query`.\n",
    "\n",
    "This process can be completed step-by-step, but it is structured into a function `build_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 8017\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 2005\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(model_name,\n",
    "                  dataset_name,\n",
    "                  input_min_text_length, \n",
    "                  input_max_text_length):\n",
    "\n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): Tokenizer model name.\n",
    "    - dataset_name (str): Name of the dataset to load.\n",
    "    - input_min_text_length (int): Minimum length of the dialogues.\n",
    "    - input_max_text_length (int): Maximum length of the dialogues.\n",
    "        \n",
    "    Returns:\n",
    "    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # load only the train dataset as this size is sufficient for this usecase.\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    \n",
    "    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n",
    "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
    "\n",
    "    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        \n",
    "        # Wrap each dialogue with the instruction.\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{sample[\"dialogue\"]}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
    "        \n",
    "        # This must be called \"query\", which is a requirement of our PPO library.\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    # Tokenize each dialogue.\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    dataset.set_format(type=\"torch\")\n",
    "    \n",
    "    # Split the dataset into train and test parts.\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
    "\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name=model_name,\n",
    "                        dataset_name=huggingface_dataset_name,\n",
    "                        input_min_text_length=200, \n",
    "                        input_max_text_length=1000)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a convenience function to pull out the number of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the adapter to the original FLAN-T5 model. In the previous lab you were adding the fully trained adapter only for inferences, so there was no need to pass LoRA configurations doing that. Now you need to pass them to the constructed PEFT model, also putting `is_trainable=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n",
    "                                              torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       '../peft-dialogue-summary-checkpoint-from-s3/', \n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16, \n",
    "                                       device_map=\"auto\",                                       \n",
    "                                       is_trainable=True)\n",
    "\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you are preparing to fine-tune the LLM using Reinforcement Learning (RL). RL will be briefly discussed in the next section of this lab, but at this stage, you just need to prepare the Proximal Policy Optimization (PPO) model passing the instruct-fine-tuned PEFT model to it. PPO will be used to optimize the RL policy against the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be updated (ValueHead + 769 params):\n",
      "\n",
      "trainable model parameters: 3539713\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,                                                               \n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               is_trainable=True)\n",
    "\n",
    "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During PPO, only a few parameters will be updated. Specifically, the parameters of the `ValueHead`. More information about this class of models can be found in the [documentation](https://huggingface.co/docs/trl/main/en/models#trl.create_reference_model). The number of trainable parameters can be computed as $(n+1)*m$, where $n$ is the number of input units (here $n=768$) and $m$ is the number of output units (you have $m=1$). The $+1$ term in the equation takes into account the bias term.\n",
    "\n",
    "Now create a frozen copy of the PPO which will not be fine-tuned - a reference model. The reference model will represent the LLM before detoxification. None of the parameters of the reference model will be updated during PPO training. This is on purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Prepare Reward Model\n",
    "\n",
    "**Reinforcement Learning (RL)** is one type of machine learning where agents take actions in an environment aimed at maximizing their cumulative rewards. The agent's behavior is defined by the **policy**. And the goal of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the **reward function**. \n",
    "\n",
    "In the [previous section](#2.1) the original policy is based on the instruct PEFT model - this is the LLM before detoxification. Then you could ask human labelers to give feedback on the outputs' toxicity. However, it can be expensive to use them for the entire fine-tuning process. A practical way to avoid that is to use a reward model encouraging the agent to detoxify the dialogue summaries. The intuitive approach would be to do some form of sentiment analysis across two classes (`nothate` and `hate`) and give a higher reward if there is higher a chance of getting class `nothate` as an output. \n",
    "\n",
    "For example, we can mention that having human labelers for the entire finetuning process can be expensive. A practical way to avoid that is to use a reward model.\n",
    "\n",
    "use feedback generated by a model\n",
    "\n",
    "You will use [Meta AI's RoBERTa-based hate speech model](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) for the reward model. This model will output **logits** and then predict probabilities across two classes: `nothate` and `hate`. The logits of the output `nothate` will be taken as a positive reward. Then, the model will be fine-tuned with PPO using those reward values.\n",
    "\n",
    "Create the instance of the required model class for the RoBERTa model. You also need to load a tokenizer to test the model. Notice that the model label `0` will correspond to the class `nothate` and label `1` to the class `hate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take some non-toxic text, tokenize it, and pass it to the model. Print the output logits, probabilities, and the corresponding reward that will be used for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [3.1140995025634766, -2.489616632461548]\n",
      "probabilities [not hate, hate]: [0.9963293671607971, 0.0036706251557916403]\n",
      "reward (high): [3.1140995025634766]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (high): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show a toxic comment.  This will have a low reward because it is more toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [-0.6921157240867615, 0.3722701370716095]\n",
      "probabilities [not hate, hate]: [0.2564722001552582, 0.7435278296470642]\n",
      "reward (low): [-0.6921157240867615]\n"
     ]
    }
   ],
   "source": [
    "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "logits = toxicity_model(toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# Get the logits for \"not hate\" - this is the reward!\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist() \n",
    "print(f'reward (low): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Hugging Face inference pipeline to simplify the code for the toxicity reward model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output:\n",
      "For non-toxic text\n",
      "[{'label': 'nothate', 'score': 3.1140995025634766}, {'label': 'hate', 'score': -2.489616632461548}]\n",
      "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.0036706249229609966}]\n",
      "For toxic text\n",
      "[{'label': 'hate', 'score': 0.3722701370716095}, {'label': 'nothate', 'score': -0.6921157240867615}]\n",
      "[{'label': 'hate', 'score': 0.7435278296470642}, {'label': 'nothate', 'score': 0.2564722001552582}]\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", \n",
    "                          model=toxicity_model_name, \n",
    "                          device=device)\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output:\")\n",
    "print(\"For non-toxic text\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
    "print(\"For toxic text\")\n",
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are the logits for both `nothate` (positive) and `hate` (negative) classes. But PPO will be using logits only of the `nothate` class as the positive reward signal used to help detoxify the LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'nothate', 'score': 3.1140995025634766}, {'label': 'hate', 'score': -2.489616632461548}]\n",
      "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.0036706249229609966}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'hate', 'score': 0.3722701370716095}, {'label': 'nothate', 'score': -0.6921157240867615}]\n",
      "[{'label': 'hate', 'score': 0.7435278296470642}, {'label': 'nothate', 'score': 0.2564722001552582}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Evaluate Toxicity\n",
    "\n",
    "To evaluate the model before and after fine-tuning/detoxification you need to set up the [toxicity evaluation metric](https://huggingface.co/spaces/evaluate-measurement/toxicity). The **toxicity score** is a decimal value between 0 and 1 where 1 is the highest toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\", \n",
    "                                    toxicity_model_name,\n",
    "                                    module_type=\"measurement\",\n",
    "                                    toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to calculate toxicity for the same sentences as in section 2.2. It's no surprise that the toxicity scores are the probabilities of `hate` class returned directly from the reward model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text:\n",
      "[0.0036706249229609966]\n",
      "\n",
      "Toxicity score for toxic text:\n",
      "[0.7435278296470642]\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    non_toxic_text\n",
    "])\n",
    "\n",
    "print(\"Toxicity score for non-toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])\n",
    "\n",
    "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
    "    toxic_text\n",
    "])\n",
    "\n",
    "print(\"\\nToxicity score for toxic text:\")\n",
    "print(toxicity_score[\"toxicity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluator can be used to compute the toxicity of the dialogues prepared in section 2.1. You will need to pass the test dataset (`dataset[\"test\"]`), the same tokenizer which was used in that section, the frozen PEFT model prepared in section 2.2, and the toxicity evaluator. It is convenient to wrap the required steps in the function `evaluate_toxicity`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_toxicity(model, \n",
    "                      toxicity_evaluator, \n",
    "                      tokenizer, \n",
    "                      dataset, \n",
    "                      num_samples):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocess the dataset and split it into train and test parts.\n",
    "\n",
    "    Parameters:\n",
    "    - model (trl model): Model to be evaluated.\n",
    "    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n",
    "    - tokenizer (transformers tokenizer): Tokenizer to be used.\n",
    "    - dataset (dataset): Input dataset for the evaluation.\n",
    "    - num_samples (int): Maximum number of samples for the evaluation.\n",
    "        \n",
    "    Returns:\n",
    "    tuple: A tuple containing two numpy.float64 values:\n",
    "    - mean (numpy.float64): Mean of the samples toxicity.\n",
    "    - std (numpy.float64): Standard deviation of the samples toxicity.\n",
    "    \"\"\"\n",
    "\n",
    "    max_new_tokens=100\n",
    "\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample[\"query\"]\n",
    "\n",
    "        if i > num_samples:\n",
    "            break\n",
    "            \n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
    "        \n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             tok_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "\n",
    "        response_token_ids = model.generate(input_ids=input_ids,\n",
    "                                            generation_config=generation_config)\n",
    "        \n",
    "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
    "\n",
    "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
    "\n",
    "    # Compute mean & std using np.\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "        \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now perform the calculation of the model toxicity before fine-tuning/detoxification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:21,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] before detox: [0.030068688242780892, 0.03622305952962595]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model, \n",
    "                                                                          toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                          tokenizer=tokenizer, \n",
    "                                                                          dataset=dataset[\"test\"], \n",
    "                                                                          num_samples=10)\n",
    "\n",
    "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Perform Fine-Tuning to Detoxify the Summaries\n",
    "Optimize a RL policy against the reward model using Proximal Policy Optimization (PPO).\n",
    "\n",
    "### 3.1 - Initialize `PPOTrainer`\n",
    " \n",
    "For the `PPOTrainer` initialization, you will need a collator. Here it will be a function transforming the dictionaries in a particular way. You can define and test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
      "Collator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"
     ]
    }
   ],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
    "print(f'Collator input: {test_data}')\n",
    "print(f'Collator output: {collator(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the configuration parameters. Load the `ppo_model` and the tokenizer. You will also load a frozen version of the model `ref_model`. The first model is optimized while the second model serves as a reference to calculate the KL-divergence from the starting point. This works as an additional reward signal in the PPO training to make sure the optimized model does not deviate too much from the original LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rg255041\\AppData\\Local\\anaconda3\\envs\\PEFT\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\rg255041\\AppData\\Local\\anaconda3\\envs\\PEFT\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=1\n",
    "mini_batch_size=4\n",
    "batch_size=16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,    \n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config, \n",
    "                         model=ppo_model, \n",
    "                         ref_model=ref_model, \n",
    "                         tokenizer=tokenizer, \n",
    "                         dataset=dataset[\"train\"], \n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Fine-Tune the Model\n",
    "\n",
    "The fine-tuning loop consists of the following main steps:\n",
    "1. Get the query responses from the policy LLM (PEFT model).\n",
    "2. Get sentiments for query/responses from hate speech RoBERTa model.\n",
    "3. Optimize policy with PPO using the (query, response, reward) triplet.\n",
    "\n",
    "The operation is running if you see the following metrics appearing:\n",
    "* `objective/kl`: minimize kl divergence,\n",
    "* `ppo/returns/mean`: maximize mean returns,\n",
    "* `ppo/policy/advantages_mean`: maximize advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:37, 97.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 33.4847526550293\n",
      "ppo/returns/mean: -0.9545789957046509\n",
      "ppo/policy/advantages_mean: 0.007002641446888447\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [03:05, 91.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 24.07400131225586\n",
      "ppo/returns/mean: -0.5079269409179688\n",
      "ppo/policy/advantages_mean: -0.004173979163169861\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [04:27, 87.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 23.20353126525879\n",
      "ppo/returns/mean: -0.40779706835746765\n",
      "ppo/policy/advantages_mean: 0.08155602216720581\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [05:53, 86.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 25.44331932067871\n",
      "ppo/returns/mean: -0.5177322626113892\n",
      "ppo/policy/advantages_mean: -0.0012789815664291382\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [07:10, 83.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 19.95479393005371\n",
      "ppo/returns/mean: -0.06337286531925201\n",
      "ppo/policy/advantages_mean: 0.004184780642390251\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [08:47, 87.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 26.836881637573242\n",
      "ppo/returns/mean: -0.5935778617858887\n",
      "ppo/policy/advantages_mean: -0.001821625977754593\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [10:06, 85.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 20.782407760620117\n",
      "ppo/returns/mean: -0.2876984477043152\n",
      "ppo/policy/advantages_mean: 0.026330024003982544\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [11:27, 83.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 25.529464721679688\n",
      "ppo/returns/mean: -0.6681752800941467\n",
      "ppo/policy/advantages_mean: 0.019549470394849777\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [13:00, 86.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 24.25278663635254\n",
      "ppo/returns/mean: -0.5816940069198608\n",
      "ppo/policy/advantages_mean: 0.02091338485479355\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [14:26, 86.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 23.937570571899414\n",
      "ppo/returns/mean: -0.4599727690219879\n",
      "ppo/policy/advantages_mean: 0.0069059282541275024\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_min_length = 100\n",
    "output_max_length = 400\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    # Break when you reach max_steps.\n",
    "    if step >= max_ppo_steps:\n",
    "        break   \n",
    "\n",
    "    prompt_tensors = batch[\"input_ids\"]\n",
    "\n",
    "    # Get response from FLAN-T5/PEFT LLM.\n",
    "    summary_tensors = []\n",
    "\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()        \n",
    "            \n",
    "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "        \n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "        \n",
    "    # This needs to be called \"response\".\n",
    "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    # Compute reward outputs.\n",
    "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]    \n",
    "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
    "\n",
    "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]    \n",
    "\n",
    "    # Run PPO step.\n",
    "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "    \n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Evaluate the Model Quantitatively\n",
    "\n",
    "Load the PPO/PEFT model back in from disk and use the test dataset split to evaluate the toxicity score of the RL-fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:23,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity [mean, std] after detox: [0.02533640875481069, 0.028664978331958697]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, \n",
    "                                                                        toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                        tokenizer=tokenizer, \n",
    "                                                                        dataset=dataset[\"test\"], \n",
    "                                                                        num_samples=10)\n",
    "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare the toxicity scores of the reference model (before detoxification) and fine-tuned model (after detoxification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage improvement of toxicity score after detoxification:\n",
      "mean: 15.74%\n",
      "std: 20.87%\n"
     ]
    }
   ],
   "source": [
    "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
    "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
    "\n",
    "print(f'Percentage improvement of toxicity score after detoxification:')\n",
    "print(f'mean: {mean_improvement*100:.2f}%')\n",
    "print(f'std: {std_improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Evaluate the Model Qualitatively\n",
    "\n",
    "Let's inspect some examples from the test dataset. You can compare the original `ref_model` to the fine-tuned/detoxified `ppo_model` using the toxicity evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:25<00:00,  4.26s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# Get response from ppo and base model.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "    \n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# Decode responses.\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# Sentiment analysis of query/response pairs before/after.\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store and review the results in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response_before</th>\n",
       "      <th>response_after</th>\n",
       "      <th>reward_before</th>\n",
       "      <th>reward_after</th>\n",
       "      <th>reward_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy thinks everyone is talking about Ashley's firing, called Richard's unprofessional.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Judy and Judy miss her boss, Richard.&lt;/s&gt;</td>\n",
       "      <td>1.170665</td>\n",
       "      <td>2.260762</td>\n",
       "      <td>1.090098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...</td>\n",
       "      <td>&lt;pad&gt; #Person1# thinks honey is not going cold turkey but she told #Person1# she can quit online. #Person1# doesn't have the willpower to quit on its own.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# says there isn't a Graphic App or enough to quit the smoker, because in some cases they've suddenly hooked about a wood to smoke at once. For the most part, they allow free smokers to smoke in public places.&lt;/s&gt;</td>\n",
       "      <td>0.958203</td>\n",
       "      <td>1.975379</td>\n",
       "      <td>1.017176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to work in an office but #Person1# can't understand the options right. #Person2# thinks #Person1# should see another job counselor and advises #Person1# to help themselves.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to work full-time. #Person2# recommends working in an office and provides practice if #Person1# does not know how. #Person2# recommends to #Person1# to this one.&lt;/s&gt;</td>\n",
       "      <td>1.817206</td>\n",
       "      <td>2.131733</td>\n",
       "      <td>0.314527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...</td>\n",
       "      <td>&lt;pad&gt; #Person1# is glad that they have reached an agreement on almost every term in their trade. The final draft has several points to bring up. #Person2# asks for some minutes and will sign the contract right now.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# and #Person1# have reached an agreement on almost every term in their trade. #Person2# presents a brief description of the shirts the order finhat or issuing. The quality standard for all others are complete.&lt;/s&gt;</td>\n",
       "      <td>3.050708</td>\n",
       "      <td>3.285194</td>\n",
       "      <td>0.234486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# about how to get to the Cross Bakery building by turn around for Boston and cross Elm. #Person2# also shows #Person1# the way when #Person1# is near Broadway.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# explains to #Person2# the way to cross the bakery building to the Cross Bakery building anyway, as #Person1# can't get more way about it.&lt;/s&gt;</td>\n",
       "      <td>2.857187</td>\n",
       "      <td>3.042414</td>\n",
       "      <td>0.185227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...</td>\n",
       "      <td>&lt;pad&gt; #Person1# is calling a Spanish airline. #Person2# helps #Person1# to call the airline office and gives #Person1# a flight number and a schedule.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# calls the airlines to confirm their flight in Spanish. The airline office will help #Person1#, which has an English-language staff.&lt;/s&gt;</td>\n",
       "      <td>1.895970</td>\n",
       "      <td>2.044639</td>\n",
       "      <td>0.148668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...</td>\n",
       "      <td>&lt;pad&gt; #Person2# likes the restaurant but not the food and mediocre service.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# they're a don't like it and #Person2# gives out details about the food that in the restaurant.&lt;/s&gt;</td>\n",
       "      <td>2.153027</td>\n",
       "      <td>2.285796</td>\n",
       "      <td>0.132770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# wants to buy a toy car for son for three hundred dollars but the price is too expensive. #Person1# suggests an affordable one. And it works out perfectly.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; # There are three toy car rentals in a bargain at the Toys Fret. #Person1# sells the car to #Person2# but fails and they can clean up the price.&lt;/s&gt;</td>\n",
       "      <td>1.341222</td>\n",
       "      <td>1.431006</td>\n",
       "      <td>0.089784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# asked #Person2# to proofread her paper. #Person2# enjoys it and thanks #Person1# for the inspiration&lt;unk&gt;.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Reviewing the paper called #Person2# to her and they tell each other how they worked for the paper.&lt;/s&gt;</td>\n",
       "      <td>2.662531</td>\n",
       "      <td>2.739327</td>\n",
       "      <td>0.076795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# that #Person2# loves the use of PC. #Person2# includes the philosophy of using PC as a means to transport the goods electronically.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# suggests folks with web access have their own personal computers. #Person1# says #Person1# says they can buy in computers, but #Person2# may be wrong.&lt;/s&gt;</td>\n",
       "      <td>2.503984</td>\n",
       "      <td>2.570383</td>\n",
       "      <td>0.066399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# offers to sell a shotgun to #Person2# for 150 yuan a piece. #Person2# suggests a volume discount if #Person1# buys 1, 000 or over and gets 10% discount. #Person2# accepts.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# sells 144 tea spoons to #Person1# for 150 yuan and offers #Person1# a 10 % discount for some customers.&lt;/s&gt;</td>\n",
       "      <td>2.484514</td>\n",
       "      <td>2.547485</td>\n",
       "      <td>0.062971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...</td>\n",
       "      <td>&lt;pad&gt; #Person1# recommends taking a coffee break from work. #Person2# also doesn't want to be sent to the scold. #Person1# appreciates #Person2#'s asking for a quick break.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Sarah is up to her neck in work. Sarah wants to start tomorrow morning. She decides that that should happen later at noon.&lt;/s&gt;</td>\n",
       "      <td>1.811468</td>\n",
       "      <td>1.867051</td>\n",
       "      <td>0.055583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# went into the airport, and #Person2# says the flight wasn't late. #Person1# feels sorry for #Person1# and will go to find out if there is any more to come.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# tells #Person2# the neles flight was doing but 12 minutes ago without landing. The next column contains #Person2#. #Person1# hires a driver.&lt;/s&gt;</td>\n",
       "      <td>2.102715</td>\n",
       "      <td>2.036143</td>\n",
       "      <td>-0.066572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# about the registration for the medical card to fill the medical records. #Person2#'ll make a medical record and recommends #Person1# to enter the pharmacy. Then #Person1# is shown the keys of the pharmacy.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# asks #Person2# to make a medical record for #Person1# to register. #Person1# must pay (10 yuan) and comes to the appointment.&lt;/s&gt;</td>\n",
       "      <td>1.630565</td>\n",
       "      <td>1.474336</td>\n",
       "      <td>-0.156230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to have this cashed in small change. #Person1# chooses the cash value of 10 hundreds and twenties, or five Hundred and Ten Tinties.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to see the cash cashed in change, but #Person2#'s close friend looks for him.&lt;/s&gt;</td>\n",
       "      <td>2.322881</td>\n",
       "      <td>2.142216</td>\n",
       "      <td>-0.180665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: &lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Amanda got a peaked hat and comments in #Person1#'s opinion about it. Amanda decides to try on the sombrero in black.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Amanda buys a cap fitted her boot, a sombrero. Amanda doesn't like caps and wants a regular hat so decided to wear one.&lt;/s&gt;</td>\n",
       "      <td>1.503376</td>\n",
       "      <td>1.318311</td>\n",
       "      <td>-0.185065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...</td>\n",
       "      <td>&lt;pad&gt; Allen thinks someone has broken in and knows the door wasn't locked. He doesn't remember what he stole from the house. Allen wants to look downstairs and see what it says.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# takes an extra part to try to help the robber prepare for the log cabin. The computer suggests they can go to the boat find it and angie delivers an extra part.&lt;/s&gt;</td>\n",
       "      <td>1.993229</td>\n",
       "      <td>1.781301</td>\n",
       "      <td>-0.211928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...</td>\n",
       "      <td>&lt;pad&gt; #Person1# wants to order some DEL internet and #Person2# recommends DEL. The service can change if the phone can't be used.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person2# recommends DEL which allows #Person1# to order and glasses are fine. Fealing this DEL it is better as the network isn't connected to #Person1#'s phone.&lt;/s&gt;</td>\n",
       "      <td>2.462580</td>\n",
       "      <td>2.041476</td>\n",
       "      <td>-0.421104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh. . . Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You...</td>\n",
       "      <td>&lt;pad&gt; #Person1# is forming a music band. #Person1# will audition for the jazz band at #Person1#'s house. #Person1# says #Person1# has musical talent. #Person1# can summon a look at #Person2#'s house the next Saturday.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; #Person1# is forming a music band and you know them playing instruments. #Person1# tells #Person2# the other musicians of the band apart from #Person1#'s agreement. #Person1# convinces #Person2# to audition at #Person1#'s home instead of #Person2#'s house. If they practice at @ #Person1#'s place, they will receive notice.&lt;/s&gt;</td>\n",
       "      <td>2.998422</td>\n",
       "      <td>2.473396</td>\n",
       "      <td>-0.525026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...</td>\n",
       "      <td>&lt;pad&gt; Alice calls Li Hong but she can't go to see Mrs. Brown tomorrow morning. #Person2# apologizes and recommends going home.&lt;/s&gt;</td>\n",
       "      <td>&lt;pad&gt; Alice shames Li Hong because she skipped her mother's meeting with her. She suggests She steer off. Li Hong takes her to her friends' house but Alice decides not to visit Mr. Brown.&lt;/s&gt;</td>\n",
       "      <td>1.521704</td>\n",
       "      <td>0.902950</td>\n",
       "      <td>-0.618754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  query  \\\n",
       "0                                                                                                                                                                   Summarize the following conversation. #Person1#: Judy, what is everybody talking about? #Person2#: Haven't you heard? Richard was fired by our manager. #Person1#: You're kidding. It can't be true. #Person2#: Believe it or not. Everybody is talking about it in the company. #Person1#: Really? I'm surprised. #Person2#: Me too. Summary: </s>   \n",
       "1   Summarize the following conversation. #Person1#: It smells like an ashtray in here! #Person2#: Hi honey! What's wrong? Why do you have that look on your face? #Person1#: What's wrong? I thought we agreed that you were gonna quit smoking. #Person2#: No! I said I was going to cut down which is very different. You can't just expect me to go cold turkey overnight! #Person1#: Look, there are other ways to quit. You can try the nicotine patch, or nicotine chewing gum. We spend a fortune on cigaret...   \n",
       "2   Summarize the following conversation. #Person1#: Could you help me figure out how to look for a job? #Person2#: We have lots of options, what type of job do you need? #Person1#: I want to work in an office. #Person2#: Do you want to work part-time or full-time? #Person1#: I want to work full-time. #Person2#: We have binders with local job listings or you can make use of the computers. OK? #Person1#: I am confused a bit but I am sure that I can figure it out. #Person2#: If you make an appoint...   \n",
       "3   Summarize the following conversation. #Person1#: Here is the final draft of our contract. I'm glad that we have reached an agreement on almost every term in our trade. #Person2#: Yes, it seems to me we have come quite a long way. However, let me take a close look at the final draft. #Person1#: Do you have some points to bring up? #Person2#: Well, everything we've discussed seems to be here. #Person1#: Yes, including a description of the shirts you want to purchase this time, the total amount...   \n",
       "4   Summarize the following conversation. #Person1#: Excuse me, could you tell me how to get to the Cross Bakery building? #Person2#: The Cross Bakery building? Oh sure. You're actually walking in the opposite direction. #Person1#: Oh, you're kidding! I thought I was heading east. #Person2#: No, east is the other direction. To get to the Bakery, you need to turn around and go three blocks to Broadway. When you get to the intersection of Broadway and Elm, you hang a left. Go straight down that st...   \n",
       "5   Summarize the following conversation. #Person1#: Hello. I want to reconfirm our flight to London. #Person2#: Yes, sir. Did you call the airline? #Person1#: Yes, I did. But I couldn't communicate with them in English. They speak only Spanish. So I need your help. #Person2#: Certainly, sir. What is the flight number and when are you leaving? #Person1#: We are taking IB 385 to London tomorrow at 1 p. m. #Person2#: Oh, I see, sir. We have the airline office inside the hotel. They have an English...   \n",
       "6   Summarize the following conversation. #Person1#: So how did you like the restaurant? #Person2#: Actually, it could have been better. #Person1#: What didn't you like about it? #Person2#: It is a new restaurant. I don't think they have their act together yet. #Person1#: What did you think about the food? #Person2#: I felt that the food was pretty mediocre. #Person1#: The service wasn't that great, either. #Person2#: I agree. The service was not good. #Person1#: Do you think that you want to tr...   \n",
       "7           Summarize the following conversation. #Person1#: What can I do for you, madam? #Person2#: I'd like to buy a toy car for my son. #Person1#: How about this one? #Person2#: It looks nice. How much is it? #Person1#: They're three hundred dollars. #Person2#: Oh, I'm afraid it's too expensive. Can you show me something cheaper? #Person1#: OK, This one is one hundred and twenty. It's the cheapest here. #Person2#: OK, I'll take it. Here's the money. #Person1#: Thank you very much. Summary: </s>   \n",
       "8                       Summarize the following conversation. #Person1#: Mom, I just finished my paper. Can you proofread it before I hand it in? #Person2#: Sure, let's take a look. Sweetie, this is terrific. Your ideas are so original. #Person1#: Thanks. #Person2#: I can tell you worked hard on it. #Person1#: I really did! I started thinking about what I wanted to say three weeks ago. #Person2#: Well, it was definitely worth all the time. #Person1#: Let's just hope my teacher agrees. Summary: </s>   \n",
       "9   Summarize the following conversation. #Person1#: Today more and more families have personal computers. People have wider range of choice to communicate with the outside world. #Person2#: Right. With the establishment of Internet and a lot of web companies, people are getting more and more dependent on the web. #Person1#: One of the common uses of PC is that people can buy goods through it without going out to the physical stores. #Person2#: Can you tell me how it is done? #Person1#: If a cus...   \n",
       "10                                                                                    Summarize the following conversation. #Person1#: How much are you asking for this? #Person2#: I'm offering them to you at 150 yuan a piece. Is that all right? #Person1#: Is tax already included in their price? #Person2#: Yes. Our price can't be matched. #Person1#: Would you consider a volume discount? #Person2#: If you buy 1, 000 or more, you'll get a 10 % discount. #Person1#: I'll accept your offer. Summary: </s>   \n",
       "11  Summarize the following conversation. #Person1#: Let's take a coffee break, shall we? #Person2#: I wish I could, but I can't. #Person1#: What keeps you so busy? You've been sitting there for hours. You've got to walk around. You just can't stay on the computer forever. #Person2#: Well, I am up to my neck in work. I've got to finish this report. Sarah needs it by noon. I don't want to be scolded if I can't finish my work by the deadline. #Person1#: I understand that, but you'd feel better if ...   \n",
       "12                                                                                                                                                                                                                                        Summarize the following conversation. #Person1#: Could you help me, Sir? My flight got in 15 minutes ago. Everyone else has picked up the luggage but mine hasn't come through. #Person2#: I'm sorry, Madam, I'll go and find out if there is any more to come. Summary: </s>   \n",
       "13  Summarize the following conversation. #Person1#: Where shall I register, please? #Person2#: Here. Do you have a registration card? #Person1#: Yes. Here you are. #Person2#: Please register your information here and pay for it. And I'll make a medical record for you. #Person1#: OK. How much do I need to pay for the registration? #Person2#: Please pay ten yuan for the registration. #Person1#: Here is my money. #Person2#: This is your registration card. Please don't lose it and bring it whenever...   \n",
       "14                                                                                                                                                                        Summarize the following conversation. #Person1#: I'd like to have this cashed, please. #Person2#: Please put you name and address here. May I see your passport? #Person1#: Yes. #Person2#: How would you like it? #Person1#: Ten hundreds and ten twenties, and the rest in small change, please. #Person2#: OK. Here you are. Summary: </s>   \n",
       "15                                                                                                                                                                                                                          Summarize the following conversation. #Person1#: Amanda, how do you like this peaked cap? #Person2#: Didn't you say you want to buy a top hat? #Person1#: But I think this one fits me Well. Why don't you try on the sombrero in black? #Person2#: I don't like caps at all. Summary: </s>   \n",
       "16  Summarize the following conversation. #Person1#: Oh, my God! What's this? #Person2#: What? #Person1#: Look! This window is open. #Person2#: Did you open it before we left? #Person1#: Are you kidding? It's winter. Why would I open it? #Person2#: I don't know. Wait. Is this yours? #Person1#: No! Oh, my God! Someone has broken into the house. #Person2#: It looks that way. That's probably why the door wasn't locked when we came in. #Person1#: I locked it when I left though. #Person2#: Yes, but t...   \n",
       "17  Summarize the following conversation. #Person1#: I would like to order some internet today. #Person2#: What kind would you like? #Person1#: What kind of internet is there? #Person2#: You can get DEL or dial-up. #Person1#: Which of those two is best? #Person2#: I would recommend DEL. #Person1#: So that one better? #Person2#: It's better because it doesn't tie up the phone. #Person1#: What do you mean by that? #Person2#: DEL isn't connected through your phone line, but dial-up is. #Person1#: S...   \n",
       "18  Summarize the following conversation. #Person1#: I'm forming a music band. #Person2#: Do you already know how to play an instrument? #Person1#: Uh. . . Yeah! I'Ve told you a thousand times that I'm learning to play the drums. Now that I know how to play well, I would like to form a rock band. #Person2#: Aside from yourself, who are the other members of the band? #Person1#: We have a guy who plays guitar, and another who plays bass. Although we still haven't found anyone to be our singer. You...   \n",
       "19  Summarize the following conversation. #Person1#: Hello? #Person2#: Hello? #Person1#: Can I speak to Li Hong, please? #Person2#: Speaking. #Person1#: Hi, Li Hong. This is Alice. #Person2#: Hi, Alice. How are you? #Person1#: Not bad. Li Hong, I am sorry that I can't go to see Mrs. Brown with you tomorrow morning. My mother is ill. I must take care of her. #Person2#: I'm sorry to hear that. You'd better stay at home. After all, we can visit Mrs. Brown later #Person1#: OK. Bye - bye. #Person2#: ...   \n",
       "\n",
       "                                                                                                                                                                                                                                     response_before  \\\n",
       "0                                                                                                                                                  <pad> Judy thinks everyone is talking about Ashley's firing, called Richard's unprofessional.</s>   \n",
       "1                                                                                     <pad> #Person1# thinks honey is not going cold turkey but she told #Person1# she can quit online. #Person1# doesn't have the willpower to quit on its own.</s>   \n",
       "2                                             <pad> #Person1# wants to work in an office but #Person1# can't understand the options right. #Person2# thinks #Person1# should see another job counselor and advises #Person1# to help themselves.</s>   \n",
       "3                         <pad> #Person1# is glad that they have reached an agreement on almost every term in their trade. The final draft has several points to bring up. #Person2# asks for some minutes and will sign the contract right now.</s>   \n",
       "4                                                  <pad> #Person1# asks #Person2# about how to get to the Cross Bakery building by turn around for Boston and cross Elm. #Person2# also shows #Person1# the way when #Person1# is near Broadway.</s>   \n",
       "5                                                                                         <pad> #Person1# is calling a Spanish airline. #Person2# helps #Person1# to call the airline office and gives #Person1# a flight number and a schedule.</s>   \n",
       "6                                                                                                                                                                    <pad> #Person2# likes the restaurant but not the food and mediocre service.</s>   \n",
       "7                                                                     <pad> #Person2# wants to buy a toy car for son for three hundred dollars but the price is too expensive. #Person1# suggests an affordable one. And it works out perfectly.</s>   \n",
       "8                                                                                                                     <pad> #Person1# asked #Person2# to proofread her paper. #Person2# enjoys it and thanks #Person1# for the inspiration<unk>.</s>   \n",
       "9                                                                            <pad> #Person1# tells #Person2# that #Person2# loves the use of PC. #Person2# includes the philosophy of using PC as a means to transport the goods electronically.</s>   \n",
       "10                                                   <pad> #Person1# offers to sell a shotgun to #Person2# for 150 yuan a piece. #Person2# suggests a volume discount if #Person1# buys 1, 000 or over and gets 10% discount. #Person2# accepts.</s>   \n",
       "11                                                                  <pad> #Person1# recommends taking a coffee break from work. #Person2# also doesn't want to be sent to the scold. #Person1# appreciates #Person2#'s asking for a quick break.</s>   \n",
       "12                                                                   <pad> #Person1# went into the airport, and #Person2# says the flight wasn't late. #Person1# feels sorry for #Person1# and will go to find out if there is any more to come.</s>   \n",
       "13  <pad> #Person1# asks #Person2# about the registration for the medical card to fill the medical records. #Person2#'ll make a medical record and recommends #Person1# to enter the pharmacy. Then #Person1# is shown the keys of the pharmacy.</s>   \n",
       "14                                                                                     <pad> #Person1# wants to have this cashed in small change. #Person1# chooses the cash value of 10 hundreds and twenties, or five Hundred and Ten Tinties.</s>   \n",
       "15                                                                                                                   <pad> Amanda got a peaked hat and comments in #Person1#'s opinion about it. Amanda decides to try on the sombrero in black.</s>   \n",
       "16                                                             <pad> Allen thinks someone has broken in and knows the door wasn't locked. He doesn't remember what he stole from the house. Allen wants to look downstairs and see what it says.</s>   \n",
       "17                                                                                                             <pad> #Person1# wants to order some DEL internet and #Person2# recommends DEL. The service can change if the phone can't be used.</s>   \n",
       "18                     <pad> #Person1# is forming a music band. #Person1# will audition for the jazz band at #Person1#'s house. #Person1# says #Person1# has musical talent. #Person1# can summon a look at #Person2#'s house the next Saturday.</s>   \n",
       "19                                                                                                                <pad> Alice calls Li Hong but she can't go to see Mrs. Brown tomorrow morning. #Person2# apologizes and recommends going home.</s>   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                   response_after  \\\n",
       "0                                                                                                                                                                                                                                                                                                 <pad> Judy and Judy miss her boss, Richard.</s>   \n",
       "1                                                                                                             <pad> #Person2# says there isn't a Graphic App or enough to quit the smoker, because in some cases they've suddenly hooked about a wood to smoke at once. For the most part, they allow free smokers to smoke in public places.</s>   \n",
       "2                                                                                                                                                     <pad> #Person1# wants to work full-time. #Person2# recommends working in an office and provides practice if #Person1# does not know how. #Person2# recommends to #Person1# to this one.</s>   \n",
       "3                                                                                                            <pad> #Person2# and #Person1# have reached an agreement on almost every term in their trade. #Person2# presents a brief description of the shirts the order finhat or issuing. The quality standard for all others are complete.</s>   \n",
       "4                                                                                                                                                                                   <pad> #Person1# explains to #Person2# the way to cross the bakery building to the Cross Bakery building anyway, as #Person1# can't get more way about it.</s>   \n",
       "5                                                                                                                                                                                         <pad> #Person1# calls the airlines to confirm their flight in Spanish. The airline office will help #Person1#, which has an English-language staff.</s>   \n",
       "6                                                                                                                                                                                                              <pad> #Person1# tells #Person2# they're a don't like it and #Person2# gives out details about the food that in the restaurant.</s>   \n",
       "7                                                                                                                                                                                      <pad> # There are three toy car rentals in a bargain at the Toys Fret. #Person1# sells the car to #Person2# but fails and they can clean up the price.</s>   \n",
       "8                                                                                                                                                                                                                                   <pad> Reviewing the paper called #Person2# to her and they tell each other how they worked for the paper.</s>   \n",
       "9                                                                                                                                                                      <pad> #Person1# suggests folks with web access have their own personal computers. #Person1# says #Person1# says they can buy in computers, but #Person2# may be wrong.</s>   \n",
       "10                                                                                                                                                                                                                    <pad> #Person2# sells 144 tea spoons to #Person1# for 150 yuan and offers #Person1# a 10 % discount for some customers.</s>   \n",
       "11                                                                                                                                                                                                           <pad> Sarah is up to her neck in work. Sarah wants to start tomorrow morning. She decides that that should happen later at noon.</s>   \n",
       "12                                                                                                                                                                             <pad> #Person1# tells #Person2# the neles flight was doing but 12 minutes ago without landing. The next column contains #Person2#. #Person1# hires a driver.</s>   \n",
       "13                                                                                                                                                                                              <pad> #Person1# asks #Person2# to make a medical record for #Person1# to register. #Person1# must pay (10 yuan) and comes to the appointment.</s>   \n",
       "14                                                                                                                                                                                                                                        <pad> #Person1# wants to see the cash cashed in change, but #Person2#'s close friend looks for him.</s>   \n",
       "15                                                                                                                                                                                                              <pad> Amanda buys a cap fitted her boot, a sombrero. Amanda doesn't like caps and wants a regular hat so decided to wear one.</s>   \n",
       "16                                                                                                                                                           <pad> #Person1# takes an extra part to try to help the robber prepare for the log cabin. The computer suggests they can go to the boat find it and angie delivers an extra part.</s>   \n",
       "17                                                                                                                                                                    <pad> #Person2# recommends DEL which allows #Person1# to order and glasses are fine. Fealing this DEL it is better as the network isn't connected to #Person1#'s phone.</s>   \n",
       "18  <pad> #Person1# is forming a music band and you know them playing instruments. #Person1# tells #Person2# the other musicians of the band apart from #Person1#'s agreement. #Person1# convinces #Person2# to audition at #Person1#'s home instead of #Person2#'s house. If they practice at @ #Person1#'s place, they will receive notice.</s>   \n",
       "19                                                                                                                                                <pad> Alice shames Li Hong because she skipped her mother's meeting with her. She suggests She steer off. Li Hong takes her to her friends' house but Alice decides not to visit Mr. Brown.</s>   \n",
       "\n",
       "    reward_before  reward_after  reward_diff  \n",
       "0        1.170665      2.260762     1.090098  \n",
       "1        0.958203      1.975379     1.017176  \n",
       "2        1.817206      2.131733     0.314527  \n",
       "3        3.050708      3.285194     0.234486  \n",
       "4        2.857187      3.042414     0.185227  \n",
       "5        1.895970      2.044639     0.148668  \n",
       "6        2.153027      2.285796     0.132770  \n",
       "7        1.341222      1.431006     0.089784  \n",
       "8        2.662531      2.739327     0.076795  \n",
       "9        2.503984      2.570383     0.066399  \n",
       "10       2.484514      2.547485     0.062971  \n",
       "11       1.811468      1.867051     0.055583  \n",
       "12       2.102715      2.036143    -0.066572  \n",
       "13       1.630565      1.474336    -0.156230  \n",
       "14       2.322881      2.142216    -0.180665  \n",
       "15       1.503376      1.318311    -0.185065  \n",
       "16       1.993229      1.781301    -0.211928  \n",
       "17       2.462580      2.041476    -0.421104  \n",
       "18       2.998422      2.473396    -0.525026  \n",
       "19       1.521704      0.902950    -0.618754  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the reward mean/median of the generated sequences you can observe a significant difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PEFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
